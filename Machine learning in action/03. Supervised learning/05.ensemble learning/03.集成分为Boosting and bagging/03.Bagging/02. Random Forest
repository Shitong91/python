
1. 定义
bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。

对于这里的随机采样有必要做进一步的介绍，这里一般采用的是自助采样法（Bootstrap sampling）,即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。


随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。
所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，
其基本思想没有脱离bagging的范畴。

2 代码实现

2.1 全代码
基于sklearn 库的

from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier

X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)

clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)
scores = cross_val_score(clf, X, y, cv=5)
scores.mean()                               
0.999...

2.2 parameter

The main parameters to adjust when using these methods is n_estimators and max_features. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node.

总结一下就是 在RandmForestClassifier 中，n_estimators and max_features是最重要的两个参数。
n_estimators: 树的数目，也就是随机要采样的子数据集的个数
一般来说，越大越好，但是太大，计算效率会降低

max_features: 特征的数目
Empirical good default values are max_features=n_features for regression problems, 
and max_features=sqrt(n_features) for classification tasks
默认的是=sqrt(n_features)，也就是为分类
